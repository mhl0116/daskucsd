{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dask + condor at UCSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "from condor_utils import make_htcondor_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06aaed99e3f14f3890637de226ed4f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>UCSDHTCondorCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scopedâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = make_htcondor_cluster(local=False, dashboard_address=13346)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://169.228.130.37:14664</li>\n",
       "  <li><b>Dashboard: </b><a href='http://169.228.130.37:13346/status' target='_blank'>http://169.228.130.37:13346/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://169.228.130.37:14664' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment for root and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f():\n",
    "    import subprocess\n",
    "\n",
    "    script=\"\"\"\n",
    "source /cvmfs/cms.cern.ch/cmsset_default.sh \n",
    "cd /cvmfs/cms.cern.ch/slc7_amd64_gcc700/cms/cmssw/CMSSW_10_5_0/ \n",
    "eval `scramv1 runtime -sh` \n",
    "cd - \n",
    "root --help\n",
    "\n",
    "cp /hadoop/cms/store/user/hmei/combineStandalone.tar.gz .\n",
    "tar -xzf combineStandalone.tar.gz\n",
    "pwd\n",
    "\n",
    "# https://root-forum.cern.ch/t/error-in-cling-insertintoautoloadingstate/29347\n",
    "# Error in cling::AutoloadingVisitor::InsertIntoAutoloadingState\n",
    "export ROOT_INCLUDE_PATH=/srv/temp/\n",
    "\n",
    "cd HiggsAnalysis/CombinedLimit/\n",
    "source ./env_standalone.sh \n",
    "combine -h\n",
    "\n",
    "#which combine\n",
    "\"\"\"\n",
    "    with open(\"temp.sh\",\"w\") as fh:\n",
    "        fh.write(script)\n",
    "    _ = subprocess.getoutput(\"chmod u+x temp.sh\")\n",
    "    cmd = \"time ./temp.sh\"\n",
    "    return subprocess.getoutput(cmd)\n",
    "#print(client.submit(f).result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup doScan locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/hmei/myWorkspace/daskucsd\n",
      "/home/users/hmei/myWorkspace/daskucsd\n",
      "(102, [[1.0], (0.9984206,), (0.9978835,), (0.9973909,), (0.9969202,), (0.99640805,), (0.9959501,), (0.9954301,), (0.9949673,), (0.9944195,), (0.9938943,), (0.99342406,), (0.9929624,), (0.99246335,), (0.9919897,), (0.9914473,), (0.9909292,), (0.9903244,), (0.98979586,), (0.98920816,), (0.98865765,), (0.98806715,), (0.98749316,), (0.9869229,), (0.9862998,), (0.9857874,), (0.98526055,), (0.9846274,), (0.9839993,), (0.9833888,), (0.982736,), (0.982196,), (0.98164475,), (0.98099476,), (0.98030204,), (0.97956413,), (0.9789049,), (0.9782431,), (0.9775532,), (0.97676224,), (0.97591954,), (0.97510004,), (0.9741523,), (0.9732765,), (0.972443,), (0.9715649,), (0.9706663,), (0.96976006,), (0.9686951,), (0.9675267,), (0.9664305,), (0.9652726,), (0.9642917,), (0.96308047,), (0.96182644,), (0.9605786,), (0.9592239,), (0.9578184,), (0.9564297,), (0.9547833,), (0.95298547,), (0.951163,), (0.9493577,), (0.9469791,), (0.94469446,), (0.94215584,), (0.93988353,), (0.9375812,), (0.9350435,), (0.93263316,), (0.92971504,), (0.9263305,), (0.9226782,), (0.91864294,), (0.9146733,), (0.9104862,), (0.90595865,), (0.90090406,), (0.89525276,), (0.8894101,), (0.8827776,), (0.8756535,), (0.8679205,), (0.8595611,), (0.84986013,), (0.8377301,), (0.82632756,), (0.8130333,), (0.79911995,), (0.7803256,), (0.76018906,), (0.7322722,), (0.70179325,), (0.66735756,), (0.6145823,), (0.5552181,), (0.48021814,), (0.3892191,), (0.27564976,), (0.14471845,), (0.00141933,), [0.0]])\n",
      "('bin dataset', 4.079119667032501, 4.079119667032501, 320)\n",
      "[#0] WARNING:InputArguments -- RooAbsPdf::fitTo(hggpdfsmrel_ttH_hgg_TTHLeptonicTag_0_101) WARNING: a likelihood fit is request of what appears to be weighted data. \n",
      "       While the estimated values of the parameters will always be calculated taking the weights into account, \n",
      "       there are multiple ways to estimate the errors on these parameter values. You are advised to make an \n",
      "       explicit choice on the error calculation: \n",
      "           - Either provide SumW2Error(kTRUE), to calculate a sum-of-weights corrected HESSE error matrix \n",
      "             (error will be proportional to the number of events)\n",
      "           - Or provide SumW2Error(kFALSE), to return errors from original HESSE error matrix\n",
      "             (which will be proportional to the sum of the weights)\n",
      "       If you want the errors to reflect the information contained in the provided dataset, choose kTRUE. \n",
      "       If you want the errors to reflect the precision you would be able to obtain with an unweighted dataset \n",
      "       with 'sum-of-weights' events, choose kFALSE.\n",
      "('bin dataset', 1.2640983602032065, 1.2640983602032065, 320)\n",
      "('bin dataset', 4.079119667032501, 4.079119667032501, 320)\n",
      "[#0] WARNING:InputArguments -- RooAbsPdf::fitTo(hggpdfsmrel_ggH_hgg_TTHLeptonicTag_0_101) WARNING: a likelihood fit is request of what appears to be weighted data. \n",
      "       While the estimated values of the parameters will always be calculated taking the weights into account, \n",
      "       there are multiple ways to estimate the errors on these parameter values. You are advised to make an \n",
      "       explicit choice on the error calculation: \n",
      "           - Either provide SumW2Error(kTRUE), to calculate a sum-of-weights corrected HESSE error matrix \n",
      "             (error will be proportional to the number of events)\n",
      "           - Or provide SumW2Error(kFALSE), to return errors from original HESSE error matrix\n",
      "             (which will be proportional to the sum of the weights)\n",
      "       If you want the errors to reflect the information contained in the provided dataset, choose kTRUE. \n",
      "       If you want the errors to reflect the precision you would be able to obtain with an unweighted dataset \n",
      "       with 'sum-of-weights' events, choose kFALSE.\n",
      "[#0] WARNING:InputArguments -- RooAbsPdf::fitTo(CMS_hgg_bkgshape_TTHLeptonicTag_0_101_ext) WARNING: a likelihood fit is request of what appears to be weighted data. \n",
      "       While the estimated values of the parameters will always be calculated taking the weights into account, \n",
      "       there are multiple ways to estimate the errors on these parameter values. You are advised to make an \n",
      "       explicit choice on the error calculation: \n",
      "           - Either provide SumW2Error(kTRUE), to calculate a sum-of-weights corrected HESSE error matrix \n",
      "             (error will be proportional to the number of events)\n",
      "           - Or provide SumW2Error(kFALSE), to return errors from original HESSE error matrix\n",
      "             (which will be proportional to the sum of the weights)\n",
      "       If you want the errors to reflect the information contained in the provided dataset, choose kTRUE. \n",
      "       If you want the errors to reflect the precision you would be able to obtain with an unweighted dataset \n",
      "       with 'sum-of-weights' events, choose kFALSE.\n",
      "cd /home/users/hmei/myWorkspace/daskucsd/models/pt_lep_binning_0_120_step1_20200316/;combine -M Significance CMS-HGG_mva_13TeV_datacard_101.txt -t -1 --expectSignal=1 > combine_101.txt ;\n",
      "/home/users/hmei/myWorkspace/daskucsd/models/pt_lep_binning_0_120_step1_20200316/combineCommand101.sh\n",
      "Significance: 0.274054 1567.43582658  mvaScore 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <TCanvas::Print>: png file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ttH_hgg_TTHLeptonicTag_0_101.png has been created\n",
      "Info in <TCanvas::Print>: pdf file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ttH_hgg_TTHLeptonicTag_0_101.pdf has been created\n",
      "Info in <TCanvas::Print>: png file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ggH_hgg_TTHLeptonicTag_0_101.png has been created\n",
      "Info in <TCanvas::Print>: pdf file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ggH_hgg_TTHLeptonicTag_0_101.pdf has been created\n",
      "Info in <TCanvas::Print>: png file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_bkg_CMS-HGG_bkg_TTHLeptonicTag_0_101.png has been created\n",
      "Info in <TCanvas::Print>: pdf file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_bkg_CMS-HGG_bkg_TTHLeptonicTag_0_101.pdf has been created\n",
      "sed: can't read CMS-HGG_bkg_TTHLeptonicTag_1_101_nbkg.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source /cvmfs/cms.cern.ch/cmsset_default.sh \n",
    "cd /cvmfs/cms.cern.ch/slc7_amd64_gcc700/cms/cmssw/CMSSW_10_5_0/ \n",
    "eval `scramv1 runtime -sh` \n",
    "cd - \n",
    "\n",
    "cd /home/users/hmei/tmp/HiggsAnalysis/CombinedLimit\n",
    "source ./env_standalone.sh \n",
    "cd -\n",
    "\n",
    "#which combine\n",
    "python doScan/doAllScans.py --low 0 --high 1 --cutIndex 101 --processTag TTHLeptonicTag --year 2020 --date 20200316 --postfix pt_lep_binning_0_120_step1 --specialCut \"(global_features[28]*mass > 120)\" --doOneBin\n",
    "#python testParse.py --low 0\n",
    "\n",
    "cat parseout_101.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/hmei/myWorkspace/daskucsd\n",
      "/home/users/hmei/myWorkspace/daskucsd\n",
      "(102, [[1.0], (0.99848974,), (0.99806327,), (0.9976782,), (0.997311,), (0.9969432,), (0.99654984,), (0.99614894,), (0.99574846,), (0.99529207,), (0.9948537,), (0.99438125,), (0.9939427,), (0.99349856,), (0.99300027,), (0.99251735,), (0.99204344,), (0.99155563,), (0.9910494,), (0.99052984,), (0.9900052,), (0.98949754,), (0.98896724,), (0.9884311,), (0.9878927,), (0.98733795,), (0.9867605,), (0.9861995,), (0.98563486,), (0.9850463,), (0.98447776,), (0.98393166,), (0.9833534,), (0.98277444,), (0.9821716,), (0.98156494,), (0.98096335,), (0.9803438,), (0.97970253,), (0.979039,), (0.97837514,), (0.9777013,), (0.97699463,), (0.9762956,), (0.9755584,), (0.9747814,), (0.97402936,), (0.97322863,), (0.97238255,), (0.9715133,), (0.9705894,), (0.9696157,), (0.9686358,), (0.9676379,), (0.96656615,), (0.9654043,), (0.96424305,), (0.9629141,), (0.96153575,), (0.96016836,), (0.9587662,), (0.9571925,), (0.9557416,), (0.95399696,), (0.9522251,), (0.9502679,), (0.94834024,), (0.9461964,), (0.9441203,), (0.9417699,), (0.9394285,), (0.93684286,), (0.9340701,), (0.9309155,), (0.92786455,), (0.9243754,), (0.920814,), (0.91664803,), (0.912,), (0.90649515,), (0.9011447,), (0.8949861,), (0.88834924,), (0.8809944,), (0.87344164,), (0.8648934,), (0.85386926,), (0.8420576,), (0.82890415,), (0.8126693,), (0.7936089,), (0.7730143,), (0.74810064,), (0.7139788,), (0.6762168,), (0.62633383,), (0.56300604,), (0.4703189,), (0.3478227,), (0.1937112,), (0.0043064,), [0.0]])\n",
      "('bin dataset', 16.055569284086232, 16.055569284086232, 320)\n",
      "[#0] WARNING:InputArguments -- RooAbsPdf::fitTo(hggpdfsmrel_ttH_hgg_TTHLeptonicTag_0_101) WARNING: a likelihood fit is request of what appears to be weighted data. \n",
      "       While the estimated values of the parameters will always be calculated taking the weights into account, \n",
      "       there are multiple ways to estimate the errors on these parameter values. You are advised to make an \n",
      "       explicit choice on the error calculation: \n",
      "           - Either provide SumW2Error(kTRUE), to calculate a sum-of-weights corrected HESSE error matrix \n",
      "             (error will be proportional to the number of events)\n",
      "           - Or provide SumW2Error(kFALSE), to return errors from original HESSE error matrix\n",
      "             (which will be proportional to the sum of the weights)\n",
      "       If you want the errors to reflect the information contained in the provided dataset, choose kTRUE. \n",
      "       If you want the errors to reflect the precision you would be able to obtain with an unweighted dataset \n",
      "       with 'sum-of-weights' events, choose kFALSE.\n",
      "('bin dataset', 2.91933614294976, 2.91933614294976, 320)\n",
      "('bin dataset', 16.055569284086232, 16.055569284086232, 320)\n",
      "[#0] WARNING:InputArguments -- RooAbsPdf::fitTo(hggpdfsmrel_ggH_hgg_TTHLeptonicTag_0_101) WARNING: a likelihood fit is request of what appears to be weighted data. \n",
      "       While the estimated values of the parameters will always be calculated taking the weights into account, \n",
      "       there are multiple ways to estimate the errors on these parameter values. You are advised to make an \n",
      "       explicit choice on the error calculation: \n",
      "           - Either provide SumW2Error(kTRUE), to calculate a sum-of-weights corrected HESSE error matrix \n",
      "             (error will be proportional to the number of events)\n",
      "           - Or provide SumW2Error(kFALSE), to return errors from original HESSE error matrix\n",
      "             (which will be proportional to the sum of the weights)\n",
      "       If you want the errors to reflect the information contained in the provided dataset, choose kTRUE. \n",
      "       If you want the errors to reflect the precision you would be able to obtain with an unweighted dataset \n",
      "       with 'sum-of-weights' events, choose kFALSE.\n",
      "[#0] WARNING:InputArguments -- RooAbsPdf::fitTo(CMS_hgg_bkgshape_TTHLeptonicTag_0_101_ext) WARNING: a likelihood fit is request of what appears to be weighted data. \n",
      "       While the estimated values of the parameters will always be calculated taking the weights into account, \n",
      "       there are multiple ways to estimate the errors on these parameter values. You are advised to make an \n",
      "       explicit choice on the error calculation: \n",
      "           - Either provide SumW2Error(kTRUE), to calculate a sum-of-weights corrected HESSE error matrix \n",
      "             (error will be proportional to the number of events)\n",
      "           - Or provide SumW2Error(kFALSE), to return errors from original HESSE error matrix\n",
      "             (which will be proportional to the sum of the weights)\n",
      "       If you want the errors to reflect the information contained in the provided dataset, choose kTRUE. \n",
      "       If you want the errors to reflect the precision you would be able to obtain with an unweighted dataset \n",
      "       with 'sum-of-weights' events, choose kFALSE.\n",
      "cd /home/users/hmei/myWorkspace/daskucsd/models/pt_lep_binning_0_120_step1_20200316/;combine -M Significance CMS-HGG_mva_13TeV_datacard_101.txt -t -1 --expectSignal=1 > combine_101.txt ;\n",
      "/home/users/hmei/myWorkspace/daskucsd/models/pt_lep_binning_0_120_step1_20200316/combineCommand101.sh\n",
      "Significance: 0.641433 4821.69788774  mvaScore 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <TCanvas::Print>: png file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ttH_hgg_TTHLeptonicTag_0_101.png has been created\n",
      "Info in <TCanvas::Print>: pdf file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ttH_hgg_TTHLeptonicTag_0_101.pdf has been created\n",
      "Info in <TCanvas::Print>: png file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ggH_hgg_TTHLeptonicTag_0_101.png has been created\n",
      "Info in <TCanvas::Print>: pdf file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_sig_CMS-HGG_sigfit_mva_ggH_hgg_TTHLeptonicTag_0_101.pdf has been created\n",
      "Info in <TCanvas::Print>: png file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_bkg_CMS-HGG_bkg_TTHLeptonicTag_0_101.png has been created\n",
      "Info in <TCanvas::Print>: pdf file /home/users/hmei/myWorkspace/daskucsd/plots/2020/20200316_pt_lep_binning_0_120_step1//fit_bkg_CMS-HGG_bkg_TTHLeptonicTag_0_101.pdf has been created\n",
      "sed: can't read CMS-HGG_bkg_TTHLeptonicTag_1_101_nbkg.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source /cvmfs/cms.cern.ch/cmsset_default.sh \n",
    "cd /cvmfs/cms.cern.ch/slc7_amd64_gcc700/cms/cmssw/CMSSW_10_5_0/ \n",
    "eval `scramv1 runtime -sh` \n",
    "cd - \n",
    "\n",
    "cd /home/users/hmei/tmp/HiggsAnalysis/CombinedLimit\n",
    "source ./env_standalone.sh \n",
    "cd -\n",
    "\n",
    "#which combine\n",
    "python doScan/doAllScans.py --low 0 --high 1 --cutIndex 101 --processTag TTHLeptonicTag --year 2020 --date 20200316 --postfix pt_lep_binning_0_120_step1 --specialCut \"(global_features[28]*mass < 120)\" --doOneBin\n",
    "#python testParse.py --low 0\n",
    "\n",
    "cat parseout_101.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import wait\n",
    "wait(futures)\n",
    "print (\"done\")\n",
    "\n",
    "#futures\n",
    "#results = client.gather(futures)\n",
    "#print (results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
